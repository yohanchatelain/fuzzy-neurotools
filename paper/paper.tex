 \documentclass[11pt,onecolumn]{article}
 \usepackage[margin=0.5in]{geometry}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage[binary-units=true]{siunitx}
\usepackage{ulem}
%\usepackage{censor}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}

\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{dingbat}
\usepackage{mathtools}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,
    filecolor=black,
    urlcolor=blue}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\makeatletter
  \def\footnoterule{\kern-3\p@
  \hrule \@width 2in \kern 2.6\p@} % the \hrule is .4pt high
\makeatother


\begin{document}

\newcommand{\fslspm}{FSL-SPM\xspace}
\newcommand{\fslafni}{FSL-AFNI\xspace}
\newcommand{\afnispm}{AFNI-SPM\xspace}
\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:} #1\color{black}\xspace}
\newcommand{\ali}[2]{\color{green}\textbf{Ali:} #1\color{black}\xspace}
\newcommand{\discuss}[1]{\uwave{#1}}


\title{Software variability in fMRI analysis:\\ comparing between-tool and numerical errors}

\author{Ali Salari$^1$, Yohan Chatelain$^1$, [other co-authors], Gregory Kiar$^2$, Tristan Glatard$^1$ \\
$^1$ Department of Computer-Science and Software Engineering, Concordia University, Montreal, Canada\\
$^2$Center for the Developing Brain, Child Mind Institute, New York, NY, USA}

\maketitle
\begin{abstract}

Numerical and software variability broadly affect structural, diffusion, and functional MRI analyses. Numerical
variability originates in software updates or code
parallelization, whereas software variability reflects discrepancies between
models implemented in different analysis software packages. While numerical
and software variability were both shown to impact analysis outcomes, the
extent to which these sources of variability compare for a given
analysis remains understudied. This work presents a comparison of
numerical and software variability for group-level and subject-level functional MRI analyses.
We reproduced a previous comparison between functional MRI analysis
software packages FSL, AFNI, and SPM, which we extended to measure
numerical variability through Monte-Carlo arithmetic.
Although We found that between-tool variability was larger than numerical variability,
we identified brain regions that numerical perturbations simulated between-tool variability.
Furthermore, we found more numerical instability in thresholded maps than unthresholded ones
and individual analysis than group-level analysis results. \tristan{Abstract to be fixed}

\end{abstract}

\section{Introduction}

Software variability has been highlighted as an important source of error
in fMRI analysis, originating from mainly two factors: Between-tool
variability denotes differences observed in results produced by different
software toolboxes --- such as FSL, AFNI and SPM --- when analyzing the same dataset
using similar methods. Numerical variability, in contrast, denotes
differences observed in results produced by the same analytical toolbox
executed in different environments – such as different operating systems or
different hardware platforms. This manuscript investigates the extent to
which both types of variability relate to each other.

Numerical variability stems from errors introduced in floating point
computations by numerical rounding, cancellation and absorption~\cite{muller2018handbook}. In
practice, the occurrence of such errors is affected by variations in
hardware, software dependencies, compilation, parallelization, and most
likely other factors. In general, the magnitude of such errors and their
variations is in the range of 1 unit in the last place (ulp) --- also known
as machine error, the difference between two successive floating-point
numbers. However, when computations depend on imprecise libraries,
numerical error may vary by several ulps. For instance, the numerical error
of the complementary error function (erfc) is larger than 3 ulps in several
mathematical libraries~\cite{zimmermann:hal-03141101}, implying that an update of this function could introduce
numerical perturbations larger than 3 ulps. Due to numerical instability,
machine error may amplify during its propagation through the analysis,
resulting in considerable loss of precision in final results. In fMRI,
numerical variability produced by Linux updates was shown to result in Dice
coefficients ranging from 0.0 to 1.0 for ICA components produced by FSL
MELODIC~\cite{Glatard2015}.

Between-tool variability is not a surprising concept for software
developers: different programs executed on identical data may obviously
produce different results. However, in fMRI, analysis programs all aim to
estimate brain activity from measured variations in the BOLD signal, which
is not expected to be measurably impacted by computational artifacts.
Therefore, the observation reported in~\cite{bowring2019exploring} that the three
main fMRI analysis toolboxes may result in marked
analytical differences, such as Dice coefficients ranging from 0.000 to
0.684 in comparisons of thresholded group-level statistic maps, has raised
substantial concern. In a follow-up study~\cite{bowring2021isolating}, the same
authors isolate the analytical components where the pipelines diverge,
highlighting a major impact of the signal model and suggesting that the
root-cause for between-tool variability depends on the analysis design and
task paradigm.  \tristan{[refer to Xinhui's pre-print].}

We investigate the relationship between numerical and between-tool
variability through two main questions: (1) how do both types of
variability compare in magnitude, and (2) is there an association between
them. The first question determines the extent to which each variability
source may impact current fMRI results, and which one should be addressed
in priority. The second question may provide deeper insights on the origins
of software variability. Conceptually, both types of variability may
reflect a single solution space shaped by local minima and
ill-conditioning, which could be explored through numerical or software
perturbations.


\section{Materials and Methods}

\subsection{fMRI analysis \& Dataset}

We replicated the fMRI analysis described as study `ds000001'
in~\cite{schonberg2012decreasing}, relying on the data publicly available
in OpenNeuro at \url{https://openneuro.org/datasets/ds000001} and using
three widely-used software packages for fMRI data processing, namely FMRIB
Software Library (FSL)~\cite{jenkinson2012fsl}, Analysis of Functional
NeuroImages (AFNI)~\cite{cox1996afni}, and Statistical Parametric
Mapping (SPM)~\cite{penny2011statistical}. We selected this dataset because
comparable analysis pipelines implemented in FSL, AFNI and SPM were already 
publicly available and extensively described in~\cite{bowring2019exploring}.
Furthermore, the work in~\cite{bowring2019exploring} already evaluated the
effect of tool variability for this dataset, which we intended to
complement with the present quantification of numerical variability.

In the selected study, 16 healthy adult subjects participated in the
balloon analog risk task~\cite{lejuez2002evaluation} to measure risk-taking
behavior over three scanning sessions~\cite{schonberg2012decreasing}. We
reused the preprocessing, first-level, and second-level analyses
implemented by~\cite{bowring2019exploring} consistently across all three
software packages. Table~\ref{table:pipeline-steps} summarizes the analytical steps in each
pipeline.


%%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{4pt}
\begin{table}[h]
    \centering
    \begin{tabular}{|c|l|c|c|c|}
        \hline
%        \multirow{2}{*}{} & \multicolumn{1}{c}{Thresholded}& & \multicolumn{1}{c}{Unthresholded}& \\
        \multicolumn{2}{|c|}{} & FSL & AFNI & SPM \\
        \hline
        {Preprocessing} & {Motion Correction}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Segmentation}                               &    &      & \checkmark  \\
        {} & {Brain Extraction (Anatomical)}              & \checkmark     & \checkmark    & \checkmark  \\
        {} & {Brain Extraction (Functional)}              &   & \checkmark     &  \\
        {} & {Intra-subject Coregistration}               & \checkmark    & \checkmark     & \checkmark \\
        {} & {Inter-subject Registration}                 & \checkmark    & \checkmark     & \checkmark \\
        {} & {Analysis Voxel Size}                        & \checkmark    & \checkmark     & \checkmark \\
        {} & {Smoothing}                                  & \checkmark    & \checkmark     & \checkmark  \\
        \hline
        {First-level} & {Model Specification}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Inclusion of 6 Motion Parameters}                               & \checkmark   &  \checkmark    & \checkmark  \\
        {} & {Model Estimation}                           & &     & \checkmark  \\
        {} & {Contrasts}                                   &  \checkmark & \checkmark     & \checkmark \\
        \hline
        {Second-level} & {Model Specification}                          & \checkmark    & \checkmark     & \checkmark  \\
        {} & {Model Estimation}                           &      &    & \checkmark  \\
        {} & {Contrasts}                                   &   & \checkmark     & \checkmark  \\
        {} & {Second-level Inference}                               &  \checkmark  &    \checkmark  & \checkmark  \\
        \hline

      \end{tabular}
    \caption{Software processing steps (adapted from~\cite{bowring2019exploring}).}
    \label{table:pipeline-steps}
\end{table}

\subsection{Fuzzy Libmath environment}

To introduce numerical noise in the analyses, we used
the Fuzzy Libmath library~\cite{salari2021accurate}, a version of the GNU
mathematical library (libmath) instrumented with Monte-Carlo arithmetic.
Monte-Carlo arithmetic simulates roundoff and catastrophic cancellation
errors by introducing a controlled amount of noise in the floating-point
operations through the following perturbation~\cite{Parker1997-qq}:

\begin{equation} \label{eq:mca_inexact}
  inexact(x) = x + 2^{e_x-t}\xi,
\end{equation}
where $e_x$ is the exponent in the floating-point representation of $x$,
$t$ is the virtual precision (the number of unperturbed bits in the
mantissa of $x$), and $\xi$ is a random uniform variable of
$(-\frac{1}{2}, \frac{1}{2})$. We introduced the perturbation using 
Verificarlo~\cite{denis2015verificarlo}, an LLVM compiler supporting Monte-Carlo 
arithmetic and other types of numerical instrumentations.

We loaded the instrumented libmath functions in the pipeline using
LD\_PRELOAD, a Linux mechanism to force-load a shared library into an
executable. This mechanism allows functions defined in Fuzzy Libmath to transparently
overload the original ones without the need to modify or recompile the
analysis pipeline.

Fuzzy Libmath introduces numerical perturbations in the values returned by
mathematical functions but not in their input values or within their
implementation. This is done by wrapping the original functions and
applying function \texttt{inexact} to their returned values.
Listing~\ref{algo:wrapper} shows an example of this wrapping for the
\texttt{log} function in single and double precision. In this wrapper, the
original function is called through \texttt{dlsym}, a function that returns
the memory address of a symbol --- in our case \texttt{RTLD\_NEXT}, the
address of the next occurrence of the function in memory. Function wrappers
are compiled with Verificarlo. Verificarlo instruments the result of the
addition between the original function output and a floating-point zero.

\lstdefinestyle{customasm}{
  belowcaptionskip=1\baselineskip,
  frame=L,
  xleftmargin=\parindent,
  language=[x86masm]Assembler,
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\itshape\color{purple!40!black},
}
\lstinputlisting[caption=Sample wrapper function (C code), label=algo:wrapper, style=customasm]{wrapper.c}

In~\cite{salari2021accurate}, Fuzzy Libmath was shown to accurately
simulate the effect of Linux operating system updates in structural
pre-processing pipelines. To validate our pipeline instrumentations for the present study, we first verified that non-instrumented
executions of the same pipeline on the same dataset led to identical
results. We also listed the pipeline
library dependencies using the \texttt{ldd} Linux utility and verified that
(1) the tested pipelines were dynamically linked to the GNU libmath library, and 
(2) there was no alternative implementation of elementary mathematical functions in the pipeline dependencies.
Finally, we verified that the use of Fuzzy libmath affected computational results.

\subsection{Data processing}

We measured between-tool (BT) variability by running the pipelines
described in~\cite{bowring2019exploring} with FSL version 5.0.10, AFNI
version 18.1.09, and SPM12 version r7771 executed with GNU/Octave version
5.2. These software versions were identical to the study
in~\cite{bowring2019exploring} except for SPM for which we used GNU/Octave
instead of MATLAB to enable mathematical function instrumentation using
Fuzzy Libmath. Indeed, MATLAB uses its own built-in mathematical functions,
which prevents the use of Fuzzy Libmath. In AFNI, we set the number of
threads to 7 even though AFNI executions
in~\cite{bowring2019exploring} were single-threaded. This was meant to
reduce the time overhead resulting from Fuzzy Libmath instrumentation. All
the analyses were conducted on the CentOS 7.3 operating system. The
computations were performed on \href{https://www.computecanada.ca}{Compute
Canada's} Béluga cluster nodes, each with 2$\times$ Intel Gold 6148 Skylake
@ 2.4 GHz (40 cores/node) CPU and 8~GB of RAM per core. We encapsulated the
above-mentioned software packages in Docker container images available at
\url{https://github.com/big-data-lab-team/fuzzy-neurotools}.

We measured numerical (within-tool -- WT) variability by running the same analyses three
times using Fuzzy Libmath with a virtual precision of $t=53$~bits for
double-precision values and $t=24$~bits for single-precision values. These
values were chosen such that the numerical perturbation simulates machine
error. The resulting samples are equally plausible estimates of
the true numerical result at the precision used by the pipelines. Moreover, to evaluate numerical perturbations at different magnitudes, 
we repeated the FSL analyses for virtual
precisions ranging from $t=1$~bit to $t=24$~bits for single-precision
and double-precision values. We also repeated the FSL analyses for virtual 
precisions ranging from $t=24$~bits to $t=53$~bits for double-precision values, having 
set the virtual precision to $t=24$~bits for single-precision values. 

We evaluated WT and BT variability for thresholded as well as unthresholded
group-level and subject-level t-statistics maps, by computing the
standard deviation of t-statistic maps across pipelines (BT) or Fuzzy Libmath samples (WT). For BT, we computed the standard deviation 
across a given pair of tools.
For WT, we computed the standard deviation across the three
Fuzzy Libmath samples of a given tool. Moreover, we computed WT variability for a pair of tools (A, B) as follows:
\begin{equation}
  \sigma_{WT(A,B)}^2 = \sigma_{WT(A)}^2 + \sigma_{WT(B)}^2,
  \label{eq:wt-pair}
\end{equation}
where $\sigma_{WT(.)}$ is the numerical variability for a given tool
and $\sigma_{WT(., .)}$ is the numerical variability for a pair of tools.

Further, from the thresholded maps, we determined regional instability
between activation clusters in the 360 regions in the Human Connectome
Project Multi-Modal Parcellation atlas version 1.0
(HCP-MMP1.0)~\cite{glasser2016multi}. For BT, we considered a region
unstable for a pair of tools if it contained activated voxels for a tool
but not for the other one. For WT, we considered a region unstable for a
pair of tools (A, B) if for tool A or tool B it contained activated voxels only for some Fuzzy Libmath
samples.

\section{Results}
All scripts and data to reproduce the results are
available in out GitHub repository at \url{https://github.com/big-data-lab-team/fuzzy-neurotools}.


\subsection{Sanity check}

We verified the correctness of our analyses by comparing our unperturbed
t-statistic group maps with the ones obtained
in~\cite{bowring2019exploring}. For FSL, we found
identical checksums. For SPM, the checksums were different but differences
were visually unnoticeable. For AFNI, the activation maps were similar
overall, however, differences were more noticeable visually. 
The observed differences remained small (see Appendix ~\ref{sec:supp-repro}) and might be due to the use of
Octave vs MATLAB in SPM and of multithreading in AFNI. We performed visual quality control of the AFNI
and SPM results for each individual subject and confirmed that \tristan{Ali: which ones?} images were
correctly registered to the MNI template.

\subsection{In group analysis, BT variability was larger than and correlated with machine error}

Table~\ref{table:pipeline-stats} reports on summary statistics for the
group-level t-statistics. For each tool pair (A, B), BT variability was
significantly larger than machine error in tool A or B, in both thresholded
and unthresholded maps (Wilcoxon signed-rank test p~\textless~$10^{-162}$
for all tests, t-test p=0 for all tests). These global differences were
confirmed regionally (Figure~\ref{fig:unthresh-maps}-\textbf{A},\textbf{B},\textbf{C}). 
In addition, BT and WT variability appeared moderately correlated (R
in [0.56, 0.58], p=0.0, Figure~\ref{fig:unthresh-maps}-\textbf{D}). Interestingly, the correlation 
was driven by a set of voxels exhibiting comparable BT and
WT values (located around the identity line in the XY plot), which, however, did not seem to have 
any spatial consistency.


\setlength{\tabcolsep}{5pt}
\begin{table}[h]
    \centering
    \begin{tabular}{cccccc|cc}
        \toprule
        && \multicolumn{4}{c|}{Group analyses} & \multicolumn{2}{c}{Subject analyses}\\
        \multirow{2}{*}{}& {} & \multicolumn{2}{c}{Thresholded} & \multicolumn{2}{c|}{Unthresholded} & \multicolumn{2}{c}{Unthresholded} \\
        % \cmidrule{3-8}
        {} & {} & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\
        \midrule
        \rowcolor{lightgray!50}
        {Between Tools} & FSL vs. SPM        &  1.282       & 0.525      & 0.443     & 0.344  & 0.366       & 0.293     \\
        \rowcolor{lightgray!50}
        {(BT)} & FSL vs. AFNI                &  1.548       & 0.616      & 0.547     & 0.441  & 0.439       & 0.352     \\
        \rowcolor{lightgray!50} 
        {} & AFNI vs. SPM                    &  1.475       & 0.672      & 0.608     & 0.477  & 0.491       & 0.381     \\
        {Within Tool} & FSL                  &  0.354       & 0.491      & 0.082     & 0.065  & 0.077       & 0.054     \\
        {(WT)}   & SPM                       &  0.252       & 0.448      & 0.054     & 0.045  & 0.048       & 0.037     \\
        {}   & AFNI                          &  0.434       & 0.524      & 0.128     & 0.135  & 0.108       & 0.131     \\
        \bottomrule
    \end{tabular}
    \caption{Voxel-wise mean and standard deviation of BT and WT variability
    in group and subject t-statistics maps. \tristan{add stats on subject-level thresholded maps}}
    \label{table:pipeline-stats}
\end{table}

\begin{figure*}[ht]
  \centering
    % \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
      \begin{subfigure}[ht]{0.9\textwidth}
      \centering
      \includegraphics[width=0.9\textwidth]{figures/std/gl-unthresh.png}
      %\caption{Standard deviation of thresholded t-statistics map on template surface}
      \end{subfigure}
      \hfill
      \begin{subfigure}[ht]{0.9\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{figures/std-corr-unthresh-plot.png}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
      \end{subfigure}
    \caption{Unthresholded group-level variability computed between tools
    (\textbf{A}), within tools at machine error (\textbf{B}), difference
    between them (\textbf{C}), and voxel-wise comparison (\textbf{D}). }
    \label{fig:unthresh-maps}
    % \end{minipage}}
  \end{figure*}

  \subsection{In subject analyses, machine error approached BT variability in some regions}

  Table~\ref{table:pipeline-stats} also reports on summary statistics for subject-level unthresholded t-statistics maps. As for group-level
  maps, BT variability was consistently larger than WT variability (Wilcoxon
  signed-rank test p~\textless~$10^{-160}$ for all tests, t-test p=0 for all
  tests). However, for some subjects, machine error (WT variability) approached and even 
  surpassed BT in some regions (Figure~\ref{fig:unthresh-maps-sbj}).  As for the group maps, BT and WT variability appeared moderately
  correlated for all subjects (R in [0.442, 0.612], p=0.0 for all
  subjects, Appendix~\ref{sec:supp-subjects}).  

  %%%%%%%%%% Var. of Unthresh sbj05%%%%%%%%
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/std/sbj05-std.png}
    %\caption{Standard deviation of thresholded t-statistics map on template surface}
    \caption{For subject with highest WT variability,
    unthresholded subject-level variability computed between tools (\textbf{A}), within tools at machine error (\textbf{B}),
     and difference between them (\textbf{C}).}
  \label{fig:unthresh-maps-sbj}
\end{figure*}

\subsection{At the precision t=17~bits, numerical variability approached BT variability in group analysis}

While the previous results were obtained for machine error, we also
evaluated numerical variability across different virtual precisions for FSL
and found that the virtual precision of t=17 bits minimized the RMSE
between BT and WT in unthresholded group maps. Figure~\ref{fig:gnp-mni} shows BT
and WT in the corresponding group maps obtained at t=17 bits, showing that
BT and WT reach comparable magnitudes in some regions. BT and WT remained
moderately correlated at this precision. \tristan{In the text, add statistics similar to Table 2.}
  
  %%%%%%%%%% plot different precisions%%%%%%%%
  \begin{figure*}[ht]
      \begin{subfigure}[ht]{\textwidth}
        \centering
        \includegraphics[width=.75\textwidth]{figures/bg_global_precision.pdf}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
        \end{subfigure}
        \hfill
        \begin{subfigure}[ht]{\textwidth}
          \centering
          \includegraphics[width=.8\textwidth]{figures/std-corr-unthresh-vp17-plot.png}
          %\caption{Standard deviation of thresholded t-statistics map on template surface}
        \end{subfigure}
        %\caption{Standard deviation of thresholded t-statistics map on template surface}
        \caption{Unthresholded group t-statistics standard deviations computed between tools (\textbf{A}),
          within tools at the virtual precision of t=17~bits (\textbf{B}), difference between them (\textbf{C}), and 
          voxel-wise comparison (\textbf{D}).}
    \label{fig:gnp-mni}
  \end{figure*}


  \subsection{Previous results are confirmed in thresholded group maps}

  Figure~\ref{fig:thresh-maps}-\textbf{A},\textbf{B},\textbf{C} compares BT
  and WT for thresholded group maps. Thresholding is an unstable operation
  that introduced high variability for both BT and WT at the edges of
  active regions. Except in these regions, BT remained consistently larger
  than WT. To measure a potential association between BT and WT, we
  measured WT instability and BT instability in each region of the
  HCP-MMP1.0 parcellation. The confusion matrices in
  Figure~\ref{fig:thresh-maps}-\textbf{D} report on such instabilities for
  the 360 tested regions. The average ratio of unstable regions was 26\%
  for BT and 9.2\% for WT, which confirmed that BT variability was larger than machine error.
  The average Cohen's kappa score between WT instability and BT instability was $\kappa=0.17$, indicating 
  a moderate agreement between WT instability and BT instability (-1 \textless $\kappa$ \textless 1, $\kappa$\leq 0 denotes chance agreement).

  %%%%%%%%%% Var. of Thresh %%%%%%%%
  \begin{figure*}[ht]
        \begin{subfigure}[ht]{\textwidth}
          \centering
          \includegraphics[width=\textwidth]{figures/std/gl-thresh.png}
          %\caption{Standard deviation of thresholded t-statistics map on template surface}
          \end{subfigure}
          \hfill
          \begin{subfigure}[ht]{\textwidth}
            \centering
            \includegraphics[width=.9\textwidth]{figures/dice_analysis-plot.png}
            %\caption{Standard deviation of thresholded t-statistics map on template surface}
          \end{subfigure}
        \centering
      \caption{Thresholded group t-statistics standard deviation maps
      computed across tools (\textbf{A}) and Fuzzy Libmath samples
      (\textbf{B}). The confusion matrices of activation instability
      in BT and WT among the 360 regions from the HCP-MMP1.0 parcellation is represented in the bottom row.}
      \label{fig:thresh-maps}
    \end{figure*}
  

\section{Discussion}

In fMRI group analyses, machine error remains an order of magnitude
smaller than between-tool variability. Group analyses have a regularization
effect toward numerical noise, which is expected to amplify as sample size
increases. Therefore, for fMRI studies with large sample sizes, machine
error can safely be neglected with respect to between-tool variability. The
recommendation made in~\cite{botvinik2020variability} to rely on ``multiverse" analyses where
multiple analysis tools are compared is therefore likely to successfully correct
for machine error in group studies.

However, machine error remains important in group analyses that are based
on a single tool, as is commonly the case in current fMRI studies. In
particular, in our study, the inherent instability of thresholding was
triggered by machine error in 9\% of 360 brain regions, which indicates
that machine error might have impacted neuroscientific conclusions related
to these regions. 

In subject analyses, machine error and between-tool variabilty can
become comparable for some subjects in some regions. This observation is particularly
relevant to the development of fMRI-based biomarkers aiming at
individualized phenotype predictions. Machine error is expected to play
a non-negligible role in such analyses, even when predictions combine
results produced by multiple tools.

The observed regularization effect of group analyses toward numerical noise 
is consistent with observations made in~\cite{kiar2020numerical} from diffusion MRI where 
connectome graph statistics were found to be substantially unstable at the subject-level 
while group distributions remained consistent. 

\tristan{discuss larger numerical noise}

No association was found between
machine error and between-tool variability at the group or subject level,
which suggests that they can be investigated separately from each other.
Even though their effects can be of comparable magnitude, machine error and
between-tool variability are distinct and most likely involve different
root causes \tristan{revise this based on correlations}.

In conclusion, our results motivate further numerical stability
investigations in fMRI analysis pipelines. Pipeline-level analyses could be
conducted to identify specific components that contribute to numerical
variability, and if possible correct them accordingly. When instability is
inherent to the analysis, sampling results distributions through numerical
perturbations might improve stability, as explored in~\cite{kiar2021data}.
Finally, pipeline-specific statistical corrections might be envisaged to
account for numerical variability.

\bibliographystyle{plain}
\bibliography{biblio}

\clearpage

\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{section}{0}

\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thesection}{S\arabic{section}}

\textbf{\centering \Large Supplemental Materials}

\section{Reproduced results}
\label{sec:supp-repro}

Figure~\ref{fig:replication-diff} shows the difference in SPM and AFNI
group analyses maps between the results in~\cite{bowring2019exploring} and
our replication. Numerical perturbations of 1 ulp are likely to have been
introduced by our replication due to the use of Octave vs MATLAB for SPM
and multithreading for AFNI. However, the group maps remained very similar
overall, which led us to conclude that our results correctly reproduced 
the ones in~\cite{bowring2019exploring}.
\begin{figure*}[ht]
  % \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
  % \centering
  \includegraphics[width=\textwidth]{figures/replication-diffs.png}
  \caption{Differences between reproduced and original results obtained in~\cite{bowring2019exploring} 
  of unthresholded group-level t-statistics for SPM (left) and AFNI (right).}
  \label{fig:replication-diff}
% \end{minipage}}
\end{figure*}

\section{BT and WT correlations for all subjects}
\label{sec:supp-subjects}

Figure~\ref{fig:unthresh-correlation-allsbj} plots the relationship between
WT and BT for each subject, showing a consistent moderate correlation between BT and
WT.

\begin{figure*}[ht]
      \centering
      \includegraphics[width=.6\textwidth]{figures/sbj-std-corr-unthresh-plot.png}
      \caption{Relationship between BT and WT variabilty for 16 subjects.}
  \label{fig:unthresh-correlation-allsbj}
\end{figure*}

\end{document}
\endinput
